{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69da364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSTIC CHECK ---\n",
      "\n",
      "1. Checking: D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_PhilEagle.csv\n",
      "   Columns Found: ['start', 'end', 'label_base', 'label_full', 'quality', 'group_id', 'selection_numbers', 'location_id', 'segment_filename', 'output_folder', 'source_audio', 'label']\n",
      "   ‚úÖ 'location_id' exists!\n",
      "\n",
      "2. Checking: D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_NoEagle.csv\n",
      "   Columns Found: ['start', 'end', 'label_base', 'label_full', 'quality', 'group_id', 'selection_numbers', 'location_id', 'segment_filename', 'output_folder', 'source_audio', 'label']\n",
      "   ‚úÖ 'location_id' exists!\n"
     ]
    }
   ],
   "source": [
    "EAGLE_PATH = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_PhilEagle.csv\"\n",
    "NOEAGLE_PATH = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_NoEagle.csv\"\n",
    "\n",
    "print(\"--- DIAGNOSTIC CHECK ---\")\n",
    "\n",
    "try:\n",
    "    # Check Eagle File\n",
    "    print(f\"\\n1. Checking: {EAGLE_PATH}\")\n",
    "    df1 = pd.read_csv(EAGLE_PATH)\n",
    "    print(f\"   Columns Found: {list(df1.columns)}\")\n",
    "    if 'location_id' in df1.columns:\n",
    "        print(\"   ‚úÖ 'location_id' exists!\")\n",
    "    else:\n",
    "        print(\"   ‚ùå 'location_id' is MISSING.\")\n",
    "\n",
    "    # Check NoEagle File\n",
    "    print(f\"\\n2. Checking: {NOEAGLE_PATH}\")\n",
    "    df2 = pd.read_csv(NOEAGLE_PATH)\n",
    "    print(f\"   Columns Found: {list(df2.columns)}\")\n",
    "    if 'location_id' in df2.columns:\n",
    "        print(\"   ‚úÖ 'location_id' exists!\")\n",
    "    else:\n",
    "        print(\"   ‚ùå 'location_id' is MISSING.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå CRITICAL ERROR READING FILE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Data...\n",
      "   Total Valid Samples: 2975 (Filtered sites < 10 samples)\n",
      "\n",
      "üéß Extracting Features (Upgraded)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2975/2975 [06:28<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üöÄ STARTING IMPROVED LOSO VALIDATION\n",
      "==================================================\n",
      "\n",
      "üìç Testing Site: Bukidnon\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1072 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[2], line 228\u001b[0m\n",
      "\u001b[0;32m    226\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X_scaled[train_idx], X_scaled[test_idx]\n",
      "\u001b[0;32m    227\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y_encoded[train_idx], y_encoded[test_idx]\n",
      "\u001b[1;32m--> 228\u001b[0m meta_test \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m]\u001b[49m \n",
      "\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1072 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.signal  # For Median Smoothing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix, \n",
    "                             precision_recall_curve, average_precision_score, auc)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===============================\n",
    "# USER SETTINGS\n",
    "# ===============================\n",
    "# 1. Paths (Using your D:/ paths)\n",
    "SEGMENTED_AUDIO_FOLDER = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios\"\n",
    "MANIFEST_EAGLE = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_PhilEagle.csv\"\n",
    "MANIFEST_NOEAGLE = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_NoEagle.csv\"\n",
    "ANNOTATION_FOLDER = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/2_SelectionTables\"\n",
    "\n",
    "# 2. Parameters\n",
    "OVERLAP_THRESHOLD = 0.1 \n",
    "MIN_SAMPLES_PER_SITE = 10 \n",
    "PREDICTION_THRESHOLD = 0.30  # Lowered from 0.5 to catch more eagles\n",
    "SMOOTHING_KERNEL = 3         # Window size for median filtering (3 segments)\n",
    "\n",
    "# ===============================\n",
    "# HELPER FUNCTIONS\n",
    "# ===============================\n",
    "def extract_audio_features(y, sr):\n",
    "    if len(y) < sr * 0.5:\n",
    "        y = np.pad(y, (0, int(sr * 0.5) - len(y)), mode='constant')\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # 1. MFCCs (Static)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    features.append(np.mean(mfccs, axis=1))\n",
    "    features.append(np.std(mfccs, axis=1))\n",
    "    \n",
    "    # 2. DELTA MFCCs (Rate of Change)\n",
    "    mfcc_delta = librosa.feature.delta(mfccs)\n",
    "    features.append(np.mean(mfcc_delta, axis=1))\n",
    "    features.append(np.std(mfcc_delta, axis=1))\n",
    "    \n",
    "    # 3. DOUBLE DELTA (Acceleration)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "    features.append(np.mean(mfcc_delta2, axis=1))\n",
    "    features.append(np.std(mfcc_delta2, axis=1))\n",
    "\n",
    "    # 4. Spectral Features (Brightness/Pitch)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_cent))\n",
    "    features.append(np.std(spec_cent))\n",
    "    \n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_bw))\n",
    "    \n",
    "    spec_roll = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_roll))\n",
    "    \n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    features.append(np.mean(zcr))\n",
    "    \n",
    "    # 5. Spectral Contrast (Texture)\n",
    "    spec_cont = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_cont, axis=1))\n",
    "    features.append(np.std(spec_cont, axis=1))\n",
    "    \n",
    "    return np.concatenate([np.array(f).flatten() for f in features])\n",
    "\n",
    "def map_label(label_full):\n",
    "    if \"NoEagle\" in label_full: return \"NoEagle\"\n",
    "    return \"Eagle\"\n",
    "\n",
    "def load_ground_truth_events(annotation_folder):\n",
    "    gt_events = {}\n",
    "    if not os.path.exists(annotation_folder): return {}\n",
    "    for f in os.listdir(annotation_folder):\n",
    "        if f.endswith(\".txt\"):\n",
    "            base_name = f.split(\".Table\")[0].split(\".txt\")[0]\n",
    "            path = os.path.join(annotation_folder, f)\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=\"\\t\", engine=\"python\", comment=\"#\")\n",
    "                df.columns = [c.strip() for c in df.columns]\n",
    "                if 'Type' in df.columns:\n",
    "                     df = df[~df['Type'].astype(str).str.contains(\"necessar|ambiguous\", case=False, na=False)]\n",
    "                events = []\n",
    "                for _, row in df.iterrows():\n",
    "                    events.append({'start': float(row['Begin Time (s)']), 'end': float(row['End Time (s)'])} )\n",
    "                gt_events[base_name] = events\n",
    "            except: pass\n",
    "    return gt_events\n",
    "\n",
    "def calculate_bootstrap_ci(y_true, y_pred, n_iterations=1000, alpha=0.95):\n",
    "    stats = []\n",
    "    if len(y_true) < 10: return 0.0, 0.0\n",
    "    for i in range(n_iterations):\n",
    "        y_true_boot, y_pred_boot = resample(y_true, y_pred, random_state=i)\n",
    "        if len(np.unique(y_true_boot)) < 2: continue\n",
    "        score = f1_score(y_true_boot, y_pred_boot, pos_label=\"Eagle\", average='binary')\n",
    "        stats.append(score)\n",
    "    if not stats: return 0.0, 0.0\n",
    "    p = ((1.0 - alpha) / 2.0) * 100\n",
    "    lower = max(0.0, np.percentile(stats, p))\n",
    "    p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n",
    "    upper = min(1.0, np.percentile(stats, p))\n",
    "    return lower, upper\n",
    "\n",
    "# ===============================\n",
    "# 1. LOAD & MERGE DATA (ROBUST)\n",
    "# ===============================\n",
    "print(\"üìä Loading Data...\")\n",
    "\n",
    "try:\n",
    "    df_eagle = pd.read_csv(MANIFEST_EAGLE, encoding='utf-8-sig')\n",
    "    df_noeagle = pd.read_csv(MANIFEST_NOEAGLE, encoding='utf-8-sig')\n",
    "    \n",
    "    # Force clean columns\n",
    "    df_eagle.columns = [c.strip() for c in df_eagle.columns]\n",
    "    df_noeagle.columns = [c.strip() for c in df_noeagle.columns]\n",
    "    \n",
    "    manifest = pd.concat([df_eagle, df_noeagle], ignore_index=True)\n",
    "    \n",
    "    # Fallback location check\n",
    "    if 'location_id' not in manifest.columns:\n",
    "        possible = [c for c in manifest.columns if 'loc' in c.lower()]\n",
    "        if possible: manifest.rename(columns={possible[0]: 'location_id'}, inplace=True)\n",
    "        else:\n",
    "            print(\"‚ùå Critical: No location column found.\")\n",
    "            exit()\n",
    "\n",
    "    # FILTER TINY SITES\n",
    "    site_counts = manifest['location_id'].value_counts()\n",
    "    valid_sites = site_counts[site_counts >= MIN_SAMPLES_PER_SITE].index.tolist()\n",
    "    manifest = manifest[manifest['location_id'].isin(valid_sites)]\n",
    "    \n",
    "    print(f\"   Total Valid Samples: {len(manifest)} (Filtered sites < {MIN_SAMPLES_PER_SITE} samples)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading manifests: {e}\")\n",
    "    exit()\n",
    "\n",
    "gt_data = load_ground_truth_events(ANNOTATION_FOLDER)\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_groups = [] \n",
    "all_metadata = []   \n",
    "\n",
    "print(\"\\nüéß Extracting Features (Upgraded)...\")\n",
    "for idx, row in tqdm(manifest.iterrows(), total=len(manifest)):\n",
    "    filename = row['segment_filename']\n",
    "    label_full = row.get('label_full', row['label'])\n",
    "    \n",
    "    # Path Logic\n",
    "    if \"NoEagle\" in label_full:\n",
    "        folder_path = \"NoEagleAudio\" \n",
    "    else:\n",
    "        location = row['location_id']\n",
    "        label_folder = label_full \n",
    "        folder_path = os.path.join(location, label_folder)\n",
    "        \n",
    "    audio_path = os.path.join(SEGMENTED_AUDIO_FOLDER, folder_path, filename)\n",
    "    \n",
    "    if os.path.exists(audio_path):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=None)\n",
    "            feat = extract_audio_features(y, sr)\n",
    "            lbl = map_label(row.get('label_base', row['label']))\n",
    "            \n",
    "            all_features.append(feat)\n",
    "            all_labels.append(lbl)\n",
    "            all_groups.append(row['location_id'])\n",
    "            \n",
    "            all_metadata.append({\n",
    "                'source_audio': row['source_audio'], \n",
    "                'start_time': row['segment_start_time'],\n",
    "                'end_time': row['segment_end_time']\n",
    "            })\n",
    "        except: pass\n",
    "\n",
    "X = np.array(all_features)\n",
    "y = np.array(all_labels)\n",
    "groups = np.array(all_groups)\n",
    "metadata = np.array(all_metadata)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "if \"Eagle\" in le.classes_:\n",
    "    eagle_idx = le.transform([\"Eagle\"])[0]\n",
    "else:\n",
    "    print(\"‚ùå Error: No 'Eagle' class found in data!\")\n",
    "    exit()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ===============================\n",
    "# 2. LOSO LOOP (CRASH-PROOF)\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING ROBUST LOSO VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check Global Balance first\n",
    "if len(np.unique(y_encoded)) < 2:\n",
    "    print(\"‚ùå CRITICAL: Loaded dataset has only 1 class. Check paths!\")\n",
    "    exit()\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "site_metrics = []\n",
    "\n",
    "y_true_global = []\n",
    "y_pred_global = []\n",
    "y_prob_global = []\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_encoded, groups=groups):\n",
    "    test_site = groups[test_idx][0]\n",
    "    \n",
    "    if test_site == \"GeneralForest\":\n",
    "        continue \n",
    "\n",
    "    print(f\"\\nüìç Testing Site: {test_site}\")\n",
    "    \n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    meta_test = metadata[test_idx] \n",
    "    \n",
    "    # --- CRASH FIX: CHECK SPLIT BALANCE ---\n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        print(f\"   ‚ö†Ô∏è SKIPPING: Training set for {test_site} has only 1 class.\")\n",
    "        continue\n",
    "\n",
    "    # Train\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict (Threshold Moving)\n",
    "    y_prob = rf.predict_proba(X_test)[:, eagle_idx]\n",
    "    y_pred = (y_prob >= PREDICTION_THRESHOLD).astype(int) \n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=eagle_idx, average='binary')\n",
    "    try: ap = average_precision_score(y_test, y_prob, pos_label=eagle_idx)\n",
    "    except: ap = 0.0\n",
    "    \n",
    "    # Event Reconstruction (Median Smoothing)\n",
    "    files_to_process = {}\n",
    "    for i, m in enumerate(meta_test):\n",
    "        fname = m['source_audio'].replace('.wav', '')\n",
    "        if fname not in files_to_process: files_to_process[fname] = []\n",
    "        is_eagle = (y_pred[i] == eagle_idx)\n",
    "        files_to_process[fname].append({'start': m['start_time'], 'end': m['end_time'], 'is_eagle': is_eagle})\n",
    "    \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for fname, segments in files_to_process.items():\n",
    "        segments.sort(key=lambda x: x['start'])\n",
    "        \n",
    "        # Apply Smoothing\n",
    "        raw_preds = [1 if s['is_eagle'] else 0 for s in segments]\n",
    "        if len(raw_preds) >= SMOOTHING_KERNEL:\n",
    "            smoothed = scipy.signal.medfilt(raw_preds, kernel_size=SMOOTHING_KERNEL)\n",
    "        else:\n",
    "            smoothed = raw_preds\n",
    "            \n",
    "        for k, val in enumerate(smoothed):\n",
    "            segments[k]['is_eagle'] = (val == 1)\n",
    "\n",
    "        pred_events = []\n",
    "        curr = None\n",
    "        for seg in segments:\n",
    "            if seg['is_eagle']:\n",
    "                if curr and (seg['start'] - curr['end'] < 0.1): curr['end'] = seg['end']\n",
    "                else: \n",
    "                    if curr: pred_events.append(curr)\n",
    "                    curr = {'start': seg['start'], 'end': seg['end']}\n",
    "            else:\n",
    "                if curr: pred_events.append(curr); curr = None\n",
    "        if curr: pred_events.append(curr)\n",
    "        \n",
    "        real_events = gt_data.get(fname, [])\n",
    "        matched = set()\n",
    "        for p in pred_events:\n",
    "            hit = False\n",
    "            for i_gt, r in enumerate(real_events):\n",
    "                intersection = max(0, min(p['end'], r['end']) - max(p['start'], r['start']))\n",
    "                coverage = intersection / (r['end'] - r['start']) if (r['end'] - r['start']) > 0 else 0\n",
    "                if coverage >= OVERLAP_THRESHOLD: hit = True; matched.add(i_gt); break\n",
    "            if hit: tp += 1\n",
    "            else: fp += 1\n",
    "        fn += len(real_events) - len(matched)\n",
    "        \n",
    "    prec_event = tp/(tp+fp) if (tp+fp)>0 else 0\n",
    "    rec_event = tp/(tp+fn) if (tp+fn)>0 else 0\n",
    "    f1_event = 2*(prec_event*rec_event)/(prec_event+rec_event) if (prec_event+rec_event)>0 else 0\n",
    "\n",
    "    print(f\"   üîπ Seg F1: {f1:.3f} | Event F1: {f1_event:.3f} | AP: {ap:.3f}\")\n",
    "    \n",
    "    site_metrics.append({\n",
    "        'site': test_site, \n",
    "        'seg_acc': acc, 'seg_f1': f1, 'seg_ap': ap,\n",
    "        'event_f1': f1_event,\n",
    "        'samples': len(y_test)\n",
    "    })\n",
    "    \n",
    "    y_true_global.extend(le.inverse_transform(y_test))\n",
    "    y_pred_global.extend(le.inverse_transform(y_pred)) \n",
    "    y_prob_global.extend(y_prob)\n",
    "\n",
    "# ===============================\n",
    "# 3. REPORTING\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä FINAL FEASIBILITY REPORT (OPTIMIZED)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not site_metrics:\n",
    "    print(\"‚ùå No valid sites.\")\n",
    "else:\n",
    "    df_res = pd.DataFrame(site_metrics)\n",
    "    print(df_res[['site', 'samples', 'seg_f1', 'event_f1']])\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Global Weighted Averages\n",
    "    global_f1 = f1_score(y_true_global, y_pred_global, pos_label=\"Eagle\", average='binary')\n",
    "    global_acc = accuracy_score(y_true_global, y_pred_global)\n",
    "    global_ap = average_precision_score(\n",
    "        [1 if y == \"Eagle\" else 0 for y in y_true_global], \n",
    "        y_prob_global\n",
    "    )\n",
    "\n",
    "    print(f\"üèÜ Global Weighted F1:       {global_f1:.3f}\")\n",
    "    print(f\"üèÜ Global Accuracy:          {global_acc:.2%}\")\n",
    "    print(f\"üèÜ Global AP (PR Area):      {global_ap:.3f}\")\n",
    "\n",
    "    print(\"\\nüîÑ Calculating Bootstrap 95% CI...\")\n",
    "    lower, upper = calculate_bootstrap_ci(y_true_global, y_pred_global)\n",
    "    print(f\"   Confidence Interval: [{lower:.3f}, {upper:.3f}]\")\n",
    "\n",
    "    cm = confusion_matrix(y_true_global, y_pred_global, labels=[\"Eagle\", \"NoEagle\"])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[\"Eagle\", \"NoEagle\"], yticklabels=[\"Eagle\", \"NoEagle\"])\n",
    "    plt.title(f'Global Confusion Matrix\\n(Optimized F1: {global_f1:.2f})')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(\n",
    "        [1 if y == \"Eagle\" else 0 for y in y_true_global], \n",
    "        y_prob_global\n",
    "    )\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Global Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
