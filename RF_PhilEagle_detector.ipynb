{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69da364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSTIC CHECK ---\n",
      "\n",
      "1. Checking: D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_PhilEagle.csv\n",
      "   Columns Found: ['start', 'end', 'label_base', 'label_full', 'quality', 'group_id', 'selection_numbers', 'location_id', 'segment_filename', 'output_folder', 'source_audio', 'label']\n",
      "   ‚úÖ 'location_id' exists!\n",
      "\n",
      "2. Checking: D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_NoEagle.csv\n",
      "   Columns Found: ['start', 'end', 'label_base', 'label_full', 'quality', 'group_id', 'selection_numbers', 'location_id', 'segment_filename', 'output_folder', 'source_audio', 'label']\n",
      "   ‚úÖ 'location_id' exists!\n"
     ]
    }
   ],
   "source": [
    "EAGLE_PATH = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_PhilEagle.csv\"\n",
    "NOEAGLE_PATH = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_NoEagle.csv\"\n",
    "\n",
    "print(\"--- DIAGNOSTIC CHECK ---\")\n",
    "\n",
    "try:\n",
    "    # Check Eagle File\n",
    "    print(f\"\\n1. Checking: {EAGLE_PATH}\")\n",
    "    df1 = pd.read_csv(EAGLE_PATH)\n",
    "    print(f\"   Columns Found: {list(df1.columns)}\")\n",
    "    if 'location_id' in df1.columns:\n",
    "        print(\"   ‚úÖ 'location_id' exists!\")\n",
    "    else:\n",
    "        print(\"   ‚ùå 'location_id' is MISSING.\")\n",
    "\n",
    "    # Check NoEagle File\n",
    "    print(f\"\\n2. Checking: {NOEAGLE_PATH}\")\n",
    "    df2 = pd.read_csv(NOEAGLE_PATH)\n",
    "    print(f\"   Columns Found: {list(df2.columns)}\")\n",
    "    if 'location_id' in df2.columns:\n",
    "        print(\"   ‚úÖ 'location_id' exists!\")\n",
    "    else:\n",
    "        print(\"   ‚ùå 'location_id' is MISSING.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå CRITICAL ERROR READING FILE: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Data...\n",
      "   Columns Found: ['start', 'end', 'label_base', 'label_full', 'quality', 'group_id', 'selection_numbers', 'location_id', 'segment_filename', 'output_folder', 'source_audio', 'label']\n",
      "   Eagle Samples: 1489\n",
      "   NoEagle Samples: 1489\n",
      "   Total Combined: 2978\n",
      "\n",
      "üéß Extracting Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2978/2978 [04:05<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üöÄ STARTING LOSO VALIDATION\n",
      "==================================================\n",
      "\n",
      "üìç Testing Site: Agusan del Sur\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1109 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[5], line 214\u001b[0m\n",
      "\u001b[0;32m    212\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X_scaled[train_idx], X_scaled[test_idx]\n",
      "\u001b[0;32m    213\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y_encoded[train_idx], y_encoded[test_idx]\n",
      "\u001b[1;32m--> 214\u001b[0m meta_test \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m]\u001b[49m \n",
      "\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n",
      "\u001b[0;32m    217\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1109 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix, \n",
    "                             precision_recall_curve, average_precision_score, auc)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===============================\n",
    "# USER SETTINGS\n",
    "# ===============================\n",
    "# 1. Location of your folders (Bukidnon, Davao, etc.)\n",
    "SEGMENTED_AUDIO_FOLDER = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios\"\n",
    "\n",
    "# 2. Your Two Manifests\n",
    "MANIFEST_EAGLE = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_PhilEagle.csv\"\n",
    "MANIFEST_NOEAGLE = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/3_SegmentedAudios/master_manifest_NoEagle.csv\"\n",
    "\n",
    "# 3. Annotation Folder (for Stitching/Event Validation)\n",
    "ANNOTATION_FOLDER = r\"D:/_3rd Year Class/1st Sem/Machine Learning/_ForLE/For DataSet/2_SelectionTables\"\n",
    "OVERLAP_THRESHOLD = 0.1 \n",
    "\n",
    "# ===============================\n",
    "# HELPER FUNCTIONS\n",
    "# ===============================\n",
    "def extract_audio_features(y, sr):\n",
    "    if len(y) < sr * 0.5:\n",
    "        y = np.pad(y, (0, int(sr * 0.5) - len(y)), mode='constant')\n",
    "    \n",
    "    features = []\n",
    "    # MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    features.append(np.mean(mfccs, axis=1))\n",
    "    features.append(np.std(mfccs, axis=1))\n",
    "    # Spectral\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_cent))\n",
    "    features.append(np.std(spec_cent))\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_bw))\n",
    "    spec_roll = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    features.append(np.mean(spec_roll))\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    features.append(np.mean(zcr))\n",
    "    \n",
    "    return np.concatenate([np.array(f).flatten() for f in features])\n",
    "\n",
    "def map_label(label_full):\n",
    "    \"\"\"Convert detailed labels to binary Eagle/NoEagle\"\"\"\n",
    "    if pd.isna(label_full):\n",
    "        return \"NoEagle\"\n",
    "    label_str = str(label_full)\n",
    "    if \"NoEagle\" in label_str:\n",
    "        return \"NoEagle\"\n",
    "    return \"Eagle\"\n",
    "\n",
    "def load_ground_truth_events(annotation_folder):\n",
    "    \"\"\"Load ground truth eagle call events from annotation files\"\"\"\n",
    "    gt_events = {}\n",
    "    if not os.path.exists(annotation_folder): \n",
    "        print(f\"‚ö†Ô∏è Annotation folder not found: {annotation_folder}\")\n",
    "        return {}\n",
    "    \n",
    "    for f in os.listdir(annotation_folder):\n",
    "        if f.endswith(\".txt\"):\n",
    "            base_name = f.split(\".Table\")[0].split(\".txt\")[0]\n",
    "            path = os.path.join(annotation_folder, f)\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=\"\\t\", engine=\"python\", comment=\"#\")\n",
    "                df.columns = [c.strip() for c in df.columns]\n",
    "                if 'Type' in df.columns:\n",
    "                    df = df[~df['Type'].astype(str).str.contains(\"necessar|ambiguous\", case=False, na=False)]\n",
    "                events = []\n",
    "                for _, row in df.iterrows():\n",
    "                    events.append({\n",
    "                        'start': float(row['Begin Time (s)']), \n",
    "                        'end': float(row['End Time (s)'])\n",
    "                    })\n",
    "                gt_events[base_name] = events\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {f}: {e}\")\n",
    "                pass\n",
    "    \n",
    "    print(f\"‚úÖ Loaded ground truth for {len(gt_events)} audio files\")\n",
    "    return gt_events\n",
    "\n",
    "def calculate_bootstrap_ci(y_true, y_pred, n_iterations=1000, alpha=0.95):\n",
    "    \"\"\"Calculate bootstrap confidence interval for F1 score\"\"\"\n",
    "    stats = []\n",
    "    if len(y_true) < 10: \n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        y_true_boot, y_pred_boot = resample(y_true, y_pred, random_state=i)\n",
    "        if len(np.unique(y_true_boot)) < 2: \n",
    "            continue\n",
    "        score = f1_score(y_true_boot, y_pred_boot, pos_label=\"Eagle\", average='binary')\n",
    "        stats.append(score)\n",
    "    \n",
    "    if not stats: \n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    p = ((1.0 - alpha) / 2.0) * 100\n",
    "    lower = max(0.0, np.percentile(stats, p))\n",
    "    p = (alpha + ((1.0 - alpha) / 2.0)) * 100\n",
    "    upper = min(1.0, np.percentile(stats, p))\n",
    "    return lower, upper\n",
    "\n",
    "# ===============================\n",
    "# 1. LOAD & MERGE DATA\n",
    "# ===============================\n",
    "print(\"=\"*60)\n",
    "print(\"ü¶Ö PHILIPPINE EAGLE DETECTION - LOSO VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä Loading Data...\")\n",
    "\n",
    "try:\n",
    "    # Load with UTF-8-BOM handling\n",
    "    df_eagle = pd.read_csv(MANIFEST_EAGLE, encoding='utf-8-sig')\n",
    "    df_noeagle = pd.read_csv(MANIFEST_NOEAGLE, encoding='utf-8-sig')\n",
    "    \n",
    "    # Clean column names\n",
    "    df_eagle.columns = [c.strip() for c in df_eagle.columns]\n",
    "    df_noeagle.columns = [c.strip() for c in df_noeagle.columns]\n",
    "    \n",
    "    # Concatenate\n",
    "    manifest = pd.concat([df_eagle, df_noeagle], ignore_index=True)\n",
    "    \n",
    "    print(f\"   Columns Found: {list(manifest.columns)}\")\n",
    "    \n",
    "    # Check for location_id column\n",
    "    if 'location_id' not in manifest.columns:\n",
    "        print(\"‚ùå CRITICAL: 'location_id' missing. Trying fallback...\")\n",
    "        possible_cols = [c for c in manifest.columns if 'loc' in c.lower()]\n",
    "        if possible_cols:\n",
    "            print(f\"   Found similar column: '{possible_cols[0]}'. Renaming it.\")\n",
    "            manifest.rename(columns={possible_cols[0]: 'location_id'}, inplace=True)\n",
    "        else:\n",
    "            print(\"   ‚ùå No location column found. Please check CSV.\")\n",
    "            exit()\n",
    "\n",
    "    print(f\"   Eagle Samples: {len(df_eagle)}\")\n",
    "    print(f\"   NoEagle Samples: {len(df_noeagle)}\")\n",
    "    print(f\"   Total Combined: {len(manifest)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading manifests: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load ground truth annotations\n",
    "gt_data = load_ground_truth_events(ANNOTATION_FOLDER)\n",
    "\n",
    "# ===============================\n",
    "# 2. FEATURE EXTRACTION\n",
    "# ===============================\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_groups = [] \n",
    "all_metadata = []\n",
    "valid_indices = []\n",
    "failed_files = []\n",
    "\n",
    "print(\"\\nüéß Extracting Features from Audio Segments...\")\n",
    "\n",
    "for idx, row in tqdm(manifest.iterrows(), total=len(manifest), desc=\"Processing\"):\n",
    "    # Get filename\n",
    "    filename = row.get('segment_filename', '')\n",
    "    if pd.isna(filename) or filename == '':\n",
    "        continue\n",
    "    \n",
    "    # ROBUST LABEL EXTRACTION\n",
    "    label_full = row.get('label_full', row.get('label', ''))\n",
    "    if pd.isna(label_full) or label_full == '':\n",
    "        continue\n",
    "    \n",
    "    label_full = str(label_full).strip()\n",
    "    \n",
    "    # ROBUST LOCATION EXTRACTION\n",
    "    location_id = row.get('location_id', 'Unknown')\n",
    "    if pd.isna(location_id):\n",
    "        location_id = 'Unknown'\n",
    "    location_id = str(location_id).strip()\n",
    "    \n",
    "    # --- PATH LOGIC ---\n",
    "    if \"NoEagle\" in label_full:\n",
    "        # Assumes NoEagle files are in \"NoEagleAudio\" folder\n",
    "        folder_path = \"NoEagleAudio\" \n",
    "    else:\n",
    "        # Assumes Eagle files are in Location/Label folder structure\n",
    "        label_folder = label_full \n",
    "        folder_path = os.path.join(location_id, label_folder)\n",
    "        \n",
    "    audio_path = os.path.join(SEGMENTED_AUDIO_FOLDER, folder_path, filename)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(audio_path):\n",
    "        failed_files.append(audio_path)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Extract features\n",
    "        feat = extract_audio_features(y, sr)\n",
    "        \n",
    "        # Map to binary label\n",
    "        lbl = map_label(row.get('label_base', row.get('label', label_full)))\n",
    "        \n",
    "        # Store everything\n",
    "        all_features.append(feat)\n",
    "        all_labels.append(lbl)\n",
    "        all_groups.append(location_id)\n",
    "        \n",
    "        all_metadata.append({\n",
    "            'source_audio': row.get('source_audio', ''), \n",
    "            'start_time': row.get('segment_start_time', 0.0),\n",
    "            'end_time': row.get('segment_end_time', 0.0)\n",
    "        })\n",
    "        \n",
    "        valid_indices.append(idx)\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_files.append(audio_path)\n",
    "        continue\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(all_features)\n",
    "y = np.array(all_labels)\n",
    "groups = np.array(all_groups)\n",
    "metadata = np.array(all_metadata)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature Extraction Complete!\")\n",
    "print(f\"   Successfully loaded: {len(X)} samples\")\n",
    "print(f\"   Failed/Missing files: {len(failed_files)}\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "\n",
    "if len(failed_files) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è First 5 failed files:\")\n",
    "    for f in failed_files[:5]:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "# Verify data\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"   ‚Ä¢ {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "unique_groups, group_counts = np.unique(groups, return_counts=True)\n",
    "print(f\"\\nüìç Samples by Location:\")\n",
    "for group, count in zip(unique_groups, group_counts):\n",
    "    percentage = (count / len(groups)) * 100\n",
    "    print(f\"   ‚Ä¢ {group}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Verify alignment\n",
    "assert len(X) == len(y) == len(groups) == len(metadata), \"‚ùå Data arrays misaligned!\"\n",
    "print(f\"\\n‚úÖ Data arrays verified and aligned!\")\n",
    "\n",
    "# Check if we have enough data\n",
    "if len(X) < 50:\n",
    "    print(\"\\n‚ùå ERROR: Not enough samples for training!\")\n",
    "    print(f\"   Found only {len(X)} samples. Need at least 50.\")\n",
    "    exit()\n",
    "\n",
    "# ===============================\n",
    "# 3. ENCODING & SCALING\n",
    "# ===============================\n",
    "print(\"\\n‚öôÔ∏è Encoding labels and scaling features...\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"   Label encoding:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"   {i}: {label}\")\n",
    "\n",
    "if \"Eagle\" in le.classes_:\n",
    "    eagle_idx = le.transform([\"Eagle\"])[0]\n",
    "else:\n",
    "    print(\"‚ùå Error: No 'Eagle' class found in data!\")\n",
    "    exit()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete!\")\n",
    "\n",
    "# ===============================\n",
    "# 4. LOSO CROSS-VALIDATION\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING LEAVE-ONE-SITE-OUT VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "site_metrics = []\n",
    "\n",
    "# Containers for Global Metrics\n",
    "y_true_global = []\n",
    "y_pred_global = []\n",
    "y_prob_global = []\n",
    "\n",
    "# Get unique sites\n",
    "unique_sites = np.unique(groups)\n",
    "print(f\"\\nüìç Will test on {len(unique_sites)} sites: {', '.join(unique_sites)}\")\n",
    "\n",
    "for train_idx, test_idx in logo.split(X_scaled, y_encoded, groups=groups):\n",
    "    test_site = groups[test_idx][0]\n",
    "    \n",
    "    # Optional: Skip specific sites\n",
    "    if test_site == \"GeneralForest\":\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipping: {test_site} (marked as general test set)\")\n",
    "        continue \n",
    "\n",
    "    print(f\"\\n\" + \"-\"*60)\n",
    "    print(f\"üìç Testing Site: {test_site}\")\n",
    "    print(f\"   Training samples: {len(train_idx)}\")\n",
    "    print(f\"   Testing samples: {len(test_idx)}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    meta_test = metadata[test_idx] \n",
    "    \n",
    "    # Check test set balance\n",
    "    test_labels = le.inverse_transform(y_test)\n",
    "    eagle_test = np.sum(test_labels == \"Eagle\")\n",
    "    noeagle_test = np.sum(test_labels == \"NoEagle\")\n",
    "    print(f\"   Test set: {eagle_test} Eagle, {noeagle_test} NoEagle\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        n_jobs=-1, \n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_prob = rf.predict_proba(X_test)[:, eagle_idx] \n",
    "    \n",
    "    # Segment-level Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=eagle_idx, average='binary')\n",
    "    \n",
    "    try: \n",
    "        ap = average_precision_score(y_test, y_prob, pos_label=eagle_idx)\n",
    "    except: \n",
    "        ap = 0.0\n",
    "    \n",
    "    # Event Reconstruction (Stitching)\n",
    "    files_to_process = {}\n",
    "    pred_labels_text = le.inverse_transform(y_pred)\n",
    "    \n",
    "    for i, m in enumerate(meta_test):\n",
    "        fname = m['source_audio'].replace('.wav', '')\n",
    "        if fname not in files_to_process: \n",
    "            files_to_process[fname] = []\n",
    "        \n",
    "        is_eagle = (pred_labels_text[i] == \"Eagle\")\n",
    "        files_to_process[fname].append({\n",
    "            'start': m['start_time'], \n",
    "            'end': m['end_time'], \n",
    "            'is_eagle': is_eagle\n",
    "        })\n",
    "    \n",
    "    # Stitch segments into events\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    \n",
    "    for fname, segments in files_to_process.items():\n",
    "        segments.sort(key=lambda x: x['start'])\n",
    "        \n",
    "        # Merge consecutive eagle segments\n",
    "        pred_events = []\n",
    "        curr = None\n",
    "        \n",
    "        for seg in segments:\n",
    "            if seg['is_eagle']:\n",
    "                if curr and (seg['start'] - curr['end'] < 0.1): \n",
    "                    # Extend current event\n",
    "                    curr['end'] = seg['end']\n",
    "                else: \n",
    "                    # Start new event\n",
    "                    if curr: \n",
    "                        pred_events.append(curr)\n",
    "                    curr = {'start': seg['start'], 'end': seg['end']}\n",
    "            else:\n",
    "                if curr: \n",
    "                    pred_events.append(curr)\n",
    "                    curr = None\n",
    "        \n",
    "        if curr: \n",
    "            pred_events.append(curr)\n",
    "        \n",
    "        # Compare with ground truth\n",
    "        real_events = gt_data.get(fname, [])\n",
    "        matched = set()\n",
    "        \n",
    "        for p in pred_events:\n",
    "            hit = False\n",
    "            for i_gt, r in enumerate(real_events):\n",
    "                intersection = max(0, min(p['end'], r['end']) - max(p['start'], r['start']))\n",
    "                coverage = intersection / (r['end'] - r['start']) if (r['end'] - r['start']) > 0 else 0\n",
    "                \n",
    "                if coverage >= OVERLAP_THRESHOLD: \n",
    "                    hit = True\n",
    "                    matched.add(i_gt)\n",
    "                    break\n",
    "            \n",
    "            if hit: \n",
    "                tp += 1\n",
    "            else: \n",
    "                fp += 1\n",
    "        \n",
    "        fn += len(real_events) - len(matched)\n",
    "    \n",
    "    # Event-level metrics\n",
    "    prec_event = tp/(tp+fp) if (tp+fp)>0 else 0\n",
    "    rec_event = tp/(tp+fn) if (tp+fn)>0 else 0\n",
    "    f1_event = 2*(prec_event*rec_event)/(prec_event+rec_event) if (prec_event+rec_event)>0 else 0\n",
    "\n",
    "    print(f\"   üîπ Segment-level F1: {f1:.3f}\")\n",
    "    print(f\"   üîπ Event-level F1: {f1_event:.3f} (TP={tp}, FP={fp}, FN={fn})\")\n",
    "    \n",
    "    # Store results\n",
    "    site_metrics.append({\n",
    "        'site': test_site, \n",
    "        'seg_acc': acc, \n",
    "        'seg_f1': f1, \n",
    "        'seg_ap': ap,\n",
    "        'event_prec': prec_event,\n",
    "        'event_rec': rec_event,\n",
    "        'event_f1': f1_event,\n",
    "        'samples': len(y_test),\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    })\n",
    "    \n",
    "    # Accumulate global predictions\n",
    "    y_true_global.extend(le.inverse_transform(y_test))\n",
    "    y_pred_global.extend(le.inverse_transform(y_pred))\n",
    "    y_prob_global.extend(y_prob)\n",
    "\n",
    "# ===============================\n",
    "# 5. FINAL REPORTING\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL FEASIBILITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_res = pd.DataFrame(site_metrics)\n",
    "print(\"\\nüìã Per-Site Results:\")\n",
    "print(df_res[['site', 'samples', 'seg_f1', 'event_f1']].to_string(index=False))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Macro average (average across sites)\n",
    "macro_f1 = df_res['seg_f1'].mean()\n",
    "macro_event_f1 = df_res['event_f1'].mean()\n",
    "print(f\"\\nüèÜ Site-Average (Macro) Metrics:\")\n",
    "print(f\"   ‚Ä¢ Segment F1: {macro_f1:.3f}\")\n",
    "print(f\"   ‚Ä¢ Event F1: {macro_event_f1:.3f}\")\n",
    "\n",
    "# Global metrics (weighted by samples)\n",
    "global_f1 = f1_score(y_true_global, y_pred_global, pos_label=\"Eagle\", average='binary')\n",
    "global_acc = accuracy_score(y_true_global, y_pred_global)\n",
    "\n",
    "try:\n",
    "    global_ap = average_precision_score(\n",
    "        [1 if y == \"Eagle\" else 0 for y in y_true_global], \n",
    "        y_prob_global\n",
    "    )\n",
    "except:\n",
    "    global_ap = 0.0\n",
    "\n",
    "print(f\"\\nüèÜ Global (Micro) Metrics:\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {global_acc:.2%}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {global_f1:.3f} ‚Üê MAIN METRIC\")\n",
    "print(f\"   ‚Ä¢ Average Precision: {global_ap:.3f}\")\n",
    "\n",
    "# Bootstrap confidence interval\n",
    "print(\"\\nüîÑ Calculating Bootstrap 95% CI...\")\n",
    "lower, upper = calculate_bootstrap_ci(y_true_global, y_pred_global)\n",
    "print(f\"   F1 Confidence Interval: [{lower:.3f}, {upper:.3f}]\")\n",
    "\n",
    "# Event-level summary\n",
    "total_tp = df_res['tp'].sum()\n",
    "total_fp = df_res['fp'].sum()\n",
    "total_fn = df_res['fn'].sum()\n",
    "global_event_prec = total_tp/(total_tp+total_fp) if (total_tp+total_fp)>0 else 0\n",
    "global_event_rec = total_tp/(total_tp+total_fn) if (total_tp+total_fn)>0 else 0\n",
    "global_event_f1 = 2*(global_event_prec*global_event_rec)/(global_event_prec+global_event_rec) if (global_event_prec+global_event_rec)>0 else 0\n",
    "\n",
    "print(f\"\\nüéØ Event Detection Performance:\")\n",
    "print(f\"   ‚Ä¢ Precision: {global_event_prec:.3f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {global_event_rec:.3f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {global_event_f1:.3f}\")\n",
    "print(f\"   ‚Ä¢ True Positives: {total_tp}\")\n",
    "print(f\"   ‚Ä¢ False Positives: {total_fp}\")\n",
    "print(f\"   ‚Ä¢ False Negatives: {total_fn}\")\n",
    "\n",
    "# Feasibility assessment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî¨ FEASIBILITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if global_f1 >= 0.85 and global_event_f1 >= 0.70:\n",
    "    print(\"üü¢ HIGHLY FEASIBLE - Excellent performance\")\n",
    "    print(\"   ‚úÖ Ready for real-world deployment\")\n",
    "elif global_f1 >= 0.75 and global_event_f1 >= 0.60:\n",
    "    print(\"üü¢ FEASIBLE - Good performance\")\n",
    "    print(\"   ‚úÖ Suitable for deployment with monitoring\")\n",
    "elif global_f1 >= 0.65:\n",
    "    print(\"üü° MODERATELY FEASIBLE - Acceptable performance\")\n",
    "    print(\"   ‚ö†Ô∏è Consider additional data or feature engineering\")\n",
    "elif global_f1 >= 0.55:\n",
    "    print(\"üü† LIMITED FEASIBILITY - Below target performance\")\n",
    "    print(\"   ‚ö†Ô∏è Significant improvement needed\")\n",
    "else:\n",
    "    print(\"üî¥ NOT FEASIBLE - Insufficient performance\")\n",
    "    print(\"   ‚ùå Major revision of approach required\")\n",
    "\n",
    "# ===============================\n",
    "# 6. VISUALIZATIONS\n",
    "# ===============================\n",
    "print(\"\\nüìä Generating visualizations...\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true_global, y_pred_global, labels=[\"Eagle\", \"NoEagle\"])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Eagle\", \"NoEagle\"], \n",
    "            yticklabels=[\"Eagle\", \"NoEagle\"],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Global Confusion Matrix\\n(Weighted F1: {global_f1:.3f})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(\n",
    "    [1 if y == \"Eagle\" else 0 for y in y_true_global], \n",
    "    y_prob_global\n",
    ")\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='darkorange', lw=2, \n",
    "         label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Global Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower left\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-site F1 comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sites = df_res['site'].values\n",
    "seg_f1s = df_res['seg_f1'].values\n",
    "event_f1s = df_res['event_f1'].values\n",
    "\n",
    "x = np.arange(len(sites))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, seg_f1s, width, label='Segment F1', color='steelblue')\n",
    "plt.bar(x + width/2, event_f1s, width, label='Event F1', color='coral')\n",
    "\n",
    "plt.xlabel('Site', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('Per-Site Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, sites, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Analysis Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
